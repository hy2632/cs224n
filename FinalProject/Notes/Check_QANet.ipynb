{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597449948627",
   "display_name": "Python 3.6.11 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import util\n",
    "\n",
    "from args_QANet import get_train_args\n",
    "\n",
    "from collections import OrderedDict\n",
    "from json import dumps\n",
    "\n",
    "from models_QANet import QANet\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from util import collate_fn, SQuAD, get_available_devices\n",
    "\n",
    "\n",
    "char_vocab_size = 1376 # 节省内存\n",
    "word_vectors = util.torch_from_json(\"data/word_emb.json\")\n",
    "model = QANet(\n",
    "    word_vectors=word_vectors,\n",
    "    char_vocab_size=char_vocab_size,\n",
    "    char_dim=200,\n",
    "    d_model=128,\n",
    "    drop_prob=0.1,\n",
    "    num_mod_blocks=5,  # 节省内存=============================\n",
    "    maximum_context_length=400)\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-->name: emb.word_emb.weight -->grad_requirs: False\n-->name: emb.char_emb.weight -->grad_requirs: True\n-->name: emb.hwy.transforms.0.weight -->grad_requirs: True\n-->name: emb.hwy.transforms.0.bias -->grad_requirs: True\n-->name: emb.hwy.transforms.1.weight -->grad_requirs: True\n-->name: emb.hwy.transforms.1.bias -->grad_requirs: True\n-->name: emb.hwy.gates.0.weight -->grad_requirs: True\n-->name: emb.hwy.gates.0.bias -->grad_requirs: True\n-->name: emb.hwy.gates.1.weight -->grad_requirs: True\n-->name: emb.hwy.gates.1.bias -->grad_requirs: True\n-->name: emb_proj.weight -->grad_requirs: True\n-->name: emb_proj.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.2.depth.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.2.sep.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.2.sep.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.3.depth.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.3.sep.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_layers.3.sep.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.0.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.0.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.1.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.1.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.2.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.2.bias -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.3.weight -->grad_requirs: True\n-->name: emb_enc.enc.conv_lns.3.bias -->grad_requirs: True\n-->name: emb_enc.enc.att.in_proj_weight -->grad_requirs: True\n-->name: emb_enc.enc.att.in_proj_bias -->grad_requirs: True\n-->name: emb_enc.enc.att.out_proj.weight -->grad_requirs: True\n-->name: emb_enc.enc.att.out_proj.bias -->grad_requirs: True\n-->name: emb_enc.enc.att_ln.weight -->grad_requirs: True\n-->name: emb_enc.enc.att_ln.bias -->grad_requirs: True\n-->name: emb_enc.enc.ffn.w1.weight -->grad_requirs: True\n-->name: emb_enc.enc.ffn.w1.bias -->grad_requirs: True\n-->name: emb_enc.enc.ffn.w2.weight -->grad_requirs: True\n-->name: emb_enc.enc.ffn.w2.bias -->grad_requirs: True\n-->name: emb_enc.enc.ffn_ln.weight -->grad_requirs: True\n-->name: emb_enc.enc.ffn_ln.bias -->grad_requirs: True\n-->name: c2q_att.c_weight -->grad_requirs: True\n-->name: c2q_att.q_weight -->grad_requirs: True\n-->name: c2q_att.cq_weight -->grad_requirs: True\n-->name: c2q_att.bias -->grad_requirs: True\n-->name: att_proj.weight -->grad_requirs: True\n-->name: att_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_lns.0.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_lns.0.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_lns.1.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.conv_lns.1.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.att.in_proj_weight -->grad_requirs: True\n-->name: mod_enc.enc.0.att.in_proj_bias -->grad_requirs: True\n-->name: mod_enc.enc.0.att.out_proj.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.att.out_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.att_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.att_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn.w1.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn.w1.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn.w2.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn.w2.bias -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.0.ffn_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_lns.0.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_lns.0.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_lns.1.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.conv_lns.1.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.att.in_proj_weight -->grad_requirs: True\n-->name: mod_enc.enc.1.att.in_proj_bias -->grad_requirs: True\n-->name: mod_enc.enc.1.att.out_proj.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.att.out_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.att_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.att_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn.w1.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn.w1.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn.w2.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn.w2.bias -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.1.ffn_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_lns.0.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_lns.0.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_lns.1.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.conv_lns.1.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.att.in_proj_weight -->grad_requirs: True\n-->name: mod_enc.enc.2.att.in_proj_bias -->grad_requirs: True\n-->name: mod_enc.enc.2.att.out_proj.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.att.out_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.att_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.att_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn.w1.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn.w1.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn.w2.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn.w2.bias -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.2.ffn_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_lns.0.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_lns.0.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_lns.1.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.conv_lns.1.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.att.in_proj_weight -->grad_requirs: True\n-->name: mod_enc.enc.3.att.in_proj_bias -->grad_requirs: True\n-->name: mod_enc.enc.3.att.out_proj.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.att.out_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.att_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.att_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn.w1.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn.w1.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn.w2.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn.w2.bias -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.3.ffn_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.0.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.0.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.0.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.1.depth.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.1.sep.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_layers.1.sep.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_lns.0.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_lns.0.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_lns.1.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.conv_lns.1.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.att.in_proj_weight -->grad_requirs: True\n-->name: mod_enc.enc.4.att.in_proj_bias -->grad_requirs: True\n-->name: mod_enc.enc.4.att.out_proj.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.att.out_proj.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.att_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.att_ln.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn.w1.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn.w1.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn.w2.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn.w2.bias -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn_ln.weight -->grad_requirs: True\n-->name: mod_enc.enc.4.ffn_ln.bias -->grad_requirs: True\n-->name: out.w1.weight -->grad_requirs: True\n-->name: out.w1.bias -->grad_requirs: True\n-->name: out.w2.weight -->grad_requirs: True\n-->name: out.w2.bias -->grad_requirs: True\n"
    }
   ],
   "source": [
    "for name, parms in model.named_parameters():\n",
    "\t    print('-->name:', name, '-->grad_requirs:', parms.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device, _ = get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuAD('./data/train.npz', True)\n",
    "train_loader = data.DataLoader(train_dataset,\n",
    "                                   batch_size=24,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=1,\n",
    "                                   collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in train_loader:  \n",
    "    cw_idxs = cw_idxs.to(device)\n",
    "    cc_idxs = cc_idxs.to(device)\n",
    "    qw_idxs = qw_idxs.to(device)\n",
    "    qc_idxs = qc_idxs.to(device)\n",
    "    batch_size = cw_idxs.size(0)\n",
    "    log_p1, log_p2 = model(\n",
    "                        cw_idxs, cc_idxs, qw_idxs,\n",
    "                        qc_idxs) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    cw_idxs = cw_idxs.to(device)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(1, 1), requires_grad=True)\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in train_loader:  \n",
    "        cw_idxs = cw_idxs.to(device)\n",
    "        cc_idxs = cc_idxs.to(device)\n",
    "        qw_idxs = qw_idxs.to(device)\n",
    "        qc_idxs = qc_idxs.to(device)\n",
    "        batch_size = cw_idxs.size(0)\n",
    "        log_p1, log_p2 = model(\n",
    "                            cw_idxs, cc_idxs, qw_idxs,\n",
    "                            qc_idxs)\n",
    "        break\n",
    "    log_p1.backward()\n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof)"
   ]
  }
 ]
}